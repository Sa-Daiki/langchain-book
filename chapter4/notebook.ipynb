{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUj2y-stek1W"
      },
      "source": [
        "# 4. LangChain解説"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4oSu5DqWgpG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3cSlNQV9ZBB"
      },
      "source": [
        "## 4-1 LangChainの概要"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqgjYwQUemjV"
      },
      "outputs": [],
      "source": [
        "# 2023年1月、LangChainのバージョン0.1がリリースされました。\n",
        "# LangChainはバージョン0.1から、破壊的変更がある場合は事前通知とマイナーバージョンアップで対応されることになりました。\n",
        "# また同時期に、langchainという1つのパッケージから、langchain-core・langchain・langchain-openaiなどのパッケージに分割されました。\n",
        "#\n",
        "# LangChainのバージョン0.1リリースの少し前までのアップデートについては、以下のスライドにまとめているので参考にしてください。\n",
        "#\n",
        "# スライド「速習：LangChainの大きなアップデート（2023年秋〜冬）」\n",
        "# https://speakerdeck.com/os1ma/su-xi-langchainnoda-kinaatupudeto-2023nian-qiu-dong\n",
        "\n",
        "!pip install langchain-core==0.1.18 langchain==0.1.5 langchain-openai==0.0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMModXMNetUP"
      },
      "source": [
        "## 4-2 Language models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3elDRjh9jVa"
      },
      "source": [
        "### LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WY3sdyngepk-"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "\n",
        "result = llm.invoke(\"自己紹介してください。\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvqkiiI-9mI9"
      },
      "source": [
        "### Chat models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLI_K6YIewJn"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    HumanMessage(content=\"こんにちは！私はジョンと言います！\"),\n",
        "    AIMessage(content=\"こんにちは、ジョンさん！どのようにお手伝いできますか？\"),\n",
        "    HumanMessage(content= \"私の名前が分かりますか？\")\n",
        "]\n",
        "\n",
        "result = chat.invoke(messages)\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLRYe3Ec9qAO"
      },
      "source": [
        "### Callbackを使ったストリーミング"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLxGXw7zgXlR"
      },
      "outputs": [],
      "source": [
        "# ストリーミングはCallbackではなく、streamメソッドで実行可能になりました。\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "messages = [HumanMessage(content=\"自己紹介してください\")]\n",
        "\n",
        "for chunk in chat.stream(messages):\n",
        "    print(chunk.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRH_6qBQnEjZ"
      },
      "source": [
        "## 4-3 Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJvnkWfA9tIT"
      },
      "source": [
        "### Prompt templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLfdULPOjUDY"
      },
      "outputs": [],
      "source": [
        "# PromptTemplateやChatPromptTemplateは、以前より簡単に使用可能になりました。\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "以下の料理のレシピを考えてください。\n",
        "\n",
        "料理名: {dish}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "result = prompt.format(dish=\"カレー\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIYyxXfj9viN"
      },
      "source": [
        "### ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUvu-76mnWBK"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"あなたは{country}料理のプロフェッショナルです。\"),\n",
        "    (\"human\", \"以下の料理のレシピを考えてください。\\n\\n料理名: {dish}\")\n",
        "])\n",
        "\n",
        "messages = chat_prompt.format_messages(country=\"イギリス\", dish=\"肉じゃが\")\n",
        "\n",
        "print(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-WsdEpEsNta"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "result = chat.invoke(messages)\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9SAbwnNwFQq"
      },
      "source": [
        "## 4-4 Output Parsers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gUPEasNNBlP"
      },
      "source": [
        "### PydanticOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEkmkZixwEdi"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "   ingredients: list[str] = Field(description=\"ingredients of the dish\")\n",
        "   steps: list[str] = Field(description=\"steps to make the dish\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aosaD-s-wExZ"
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers import PydanticOutputParser\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Recipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blhjs33M-AHZ"
      },
      "outputs": [],
      "source": [
        "format_instructions = parser.get_format_instructions()\n",
        "\n",
        "print(format_instructions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21EfCn8b-AFY"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate.from_template(\n",
        "    template=\"\"\"料理のレシピを考えてください。\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "料理名: {dish}\n",
        "\"\"\",\n",
        "    partial_variables={\"format_instructions\": format_instructions},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5Y0KvYk-ADU"
      },
      "outputs": [],
      "source": [
        "formatted_prompt = prompt.format(dish=\"カレー\")\n",
        "\n",
        "print(formatted_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gQjKsGy9__B"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "messages = [HumanMessage(content=formatted_prompt)]\n",
        "output = chat.invoke(messages)\n",
        "\n",
        "print(output.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZY4cBqNZ9_1x"
      },
      "outputs": [],
      "source": [
        "recipe = parser.invoke(output.content)\n",
        "print(recipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUVnoApz-v9O"
      },
      "source": [
        "## 4-5 Chains"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU0czqQn-xgL"
      },
      "source": [
        "### LLMChain―PromptTemplate・Language model・OutputParserをつなぐ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpPIlgjpyroZ"
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "   ingredients: list[str] = Field(description=\"ingredients of the dish\")\n",
        "   steps: list[str] = Field(description=\"steps to make the dish\")\n",
        "\n",
        "output_parser = PydanticOutputParser(pydantic_object=Recipe)\n",
        "\n",
        "prompt = PromptTemplate.from_template(\n",
        "    template=\"\"\"料理のレシピを考えてください。\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "料理名: {dish}\n",
        "\"\"\",\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jys6cH41C4NW"
      },
      "outputs": [],
      "source": [
        "# 2023年10月後半頃から、LangChainでは「LangChain Expression Language (LCEL)」を使う実装が標準的となりました。\n",
        "# LCELではプロンプトやLLMを `|` で繋げて書き、処理の連鎖 (Chain) を実装します。\n",
        "# 以後のサンプルコードでは、LCELを使用します。\n",
        "#\n",
        "# LCELの概要は、公式ドキュメントや以下の記事を参考にしてください。\n",
        "#\n",
        "# 記事「LangChain の新記法「LangChain Expression Language (LCEL)」入門」\n",
        "# https://zenn.dev/os1ma/articles/acd3472c3a6755\n",
        "\n",
        "chain = prompt | model | output_parser\n",
        "\n",
        "recipe = chain.invoke({\"dish\": \"カレー\"})\n",
        "\n",
        "print(type(recipe))\n",
        "print(recipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCt-YzvX-z4k"
      },
      "source": [
        "### SimpleSequentialChain―ChainとChainをつなぐ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcL-GWtO-0v4"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "cot_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"以下の質問に回答してください。\n",
        "\n",
        "質問: {question}\n",
        "\n",
        "ステップバイステップで考えましょう。\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "cot_chain = (\n",
        "    {\"question\": RunnablePassthrough()}\n",
        "    | cot_prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGTWjr3rKBCX"
      },
      "outputs": [],
      "source": [
        "summarize_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"以下の文章を結論だけ一言に要約してください。\n",
        "\n",
        "{input}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "summarize_chain = (\n",
        "    {\"input\": RunnablePassthrough()}\n",
        "    | summarize_prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYGaG5tuKEVX"
      },
      "outputs": [],
      "source": [
        "cot_summarize_chain = cot_chain | summarize_chain\n",
        "\n",
        "result = cot_summarize_chain.invoke(\n",
        "   \"私は市場に行って10個のリンゴを買いました。隣人に2つ、修理工に2つ渡しました。それから5つのリンゴを買って1つ食べました。残りは何個ですか？\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ofO7A5J5Eta"
      },
      "source": [
        "上記の入力は [Chain-of-Thoughtプロンプティング | Prompt Engineering Guide](https://www.promptingguide.ai/jp/techniques/cot) から引用しました。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jpeYc6ARqBz"
      },
      "source": [
        "## 4-6 Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yOh1zrMRrbn"
      },
      "source": [
        "### ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WugAW9-LKGJ0"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
        "conversation = ConversationChain(\n",
        "    llm=chat,\n",
        "    memory=ConversationBufferMemory()\n",
        ")\n",
        "\n",
        "while True:\n",
        "    user_message = input(\"You: \")\n",
        "    ai_message = conversation.predict(input=user_message)\n",
        "    print(f\"AI: {ai_message}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asdwuUioB227"
      },
      "outputs": [],
      "source": [
        "# ChatOpenAIとConversationBufferMemoryをLCELで使用する例は次のようになります。\n",
        "\n",
        "from operator import itemgetter\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "\n",
        "chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
        "    )\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "while True:\n",
        "    user_message = input(\"You: \")\n",
        "    inputs = {\"input\": user_message}\n",
        "\n",
        "    ai_message = chain.invoke(inputs)\n",
        "    memory.save_context(inputs, {\"output\": ai_message})\n",
        "\n",
        "    print(ai_message)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
